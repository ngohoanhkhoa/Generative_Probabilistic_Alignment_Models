{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7376,"status":"ok","timestamp":1646643870695,"user":{"displayName":"Oliver NGO","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17007837836741896241"},"user_tz":-60},"id":"cIHUJ8w_2r5U","outputId":"90d0947f-8445-4e12-cb60-dbd604815ede"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","Requirement already satisfied: waiting in /usr/local/lib/python3.7/dist-packages (1.4.1)\n","mkdir: cannot create directory ‘/content/fastalign’: File exists\n","-- Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) \n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/fastalign\n","make: Entering directory '/content/fastalign'\n","make[1]: Entering directory '/content/fastalign'\n","make[2]: Entering directory '/content/fastalign'\n","make[2]: Leaving directory '/content/fastalign'\n","[ 50%] Built target atools\n","make[2]: Entering directory '/content/fastalign'\n","make[2]: Leaving directory '/content/fastalign'\n","[100%] Built target fast_align\n","make[1]: Leaving directory '/content/fastalign'\n","make: Leaving directory '/content/fastalign'\n"]}],"source":["!pip install sentencepiece\n","!pip install waiting\n","\n","!mkdir /content/fastalign\n","!cmake -B/content/fastalign -H/content/drive/MyDrive/tools/fast_align\n","!make -C/content/fastalign"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-ij2cdhm-93"},"outputs":[],"source":["from IPython.lib.display import ScribdDocument\n","import os\n","import sentencepiece as spm\n","from waiting import wait\n","import threading, subprocess\n","import concurrent.futures\n","import time\n","\n","def get_dictionary(file_name):\n","    word_dict = {}\n","    char_dict = {}\n","    with open(file_name, 'r') as file_in:\n","        for line in file_in:\n","            for word in line.split():\n","                if word not in word_dict:\n","                    word_dict[word] = 1\n","                else:\n","                    word_dict[word] += 1\n","                \n","    for word in word_dict:\n","        for char in word:\n","            if char not in char_dict:\n","                char_dict[char] = 1\n","            else:\n","                char_dict[char] += 1\n","    \n","    if not os.path.isfile(file_name + '.vocab_word'):\n","        with open(file_name + '.vocab_word', 'w+') as file_dict:\n","            for word in word_dict:\n","                file_dict.write(word + ' ' + str(word_dict[word]) + '\\n')\n","    \n","    if not os.path.isfile(file_name + '.vocab_char'):\n","        with open(file_name + '.vocab_char', 'w+') as file_dict:\n","            for char in char_dict:\n","                file_dict.write(char + ' ' + str(char_dict[char]) + '\\n')\n","\n","    return len(word_dict), len(char_dict)\n","\n","def get_tokenized_corpus(input, model_type, vocab_size=100, timeout_seconds=10):\n","    model_prefix= input + '_' + model_type\n","    if model_type in ['bpe','unigram']:\n","        model_prefix+= '_' + str(vocab_size)\n","\n","    if not os.path.isfile(model_prefix+'.model'):\n","        print(\"Training Sentencepiece:\", model_prefix)\n","        cmd = \"--input=%s --model_prefix=%s --model_type=%s --vocab_size=%d\" % (input, model_prefix, model_type, vocab_size)\n","        spm.SentencePieceTrainer.train(cmd)\n","\n","    wait(lambda: os.path.isfile(model_prefix+'.model'), timeout_seconds=timeout_seconds, waiting_for=\"SentencePiece creates .model\")\n","    vocab_size_ = vocab_size\n","    \n","    # if not os.path.isfile(model_prefix) and not os.path.isfile(\"/\".join(model_prefix.split('/')[:-1]) + '/' + model_prefix.split('/')[-1].replace('_train', '')):\n","    if not os.path.isfile(model_prefix):\n","        sp = spm.SentencePieceProcessor()\n","        sp.load(model_prefix + '.model')\n","\n","        file_in = open(input, \"r\")\n","\n","        with open(model_prefix, \"w+\") as file_out:\n","            for line in file_in:\n","                file_out.write(\" \".join(sp.encode_as_pieces(line)) + \"\\n\" )\n","    \n","        vocab_size_ = sp.GetPieceSize()\n","    return model_prefix, vocab_size_\n","\n","def token_2_word_alignment(src, tgt, align_file, is_word=[False, False]): \n","    if not os.path.isfile(align_file+'.2word'):\n","        with open(src, 'r') as src_file, open(tgt, 'r') as tgt_file, open(align_file, 'r') as file_in, open(align_file+'.2word', 'w+') as file_out:\n","            for line_src, line_tgt, line_in in zip(src_file, tgt_file, file_in):\n","\n","                line_srcs = line_src.split()\n","                if is_word[0]:\n","                    line_srcs = ['▁'+tok for tok in line_srcs]\n","\n","                line_tgts = line_tgt.split()\n","                if is_word[1]:\n","                    line_tgts = ['▁'+tok for tok in line_tgts]\n","                \n","                word_idx = 0\n","                token2word_src = {}\n","                for idx, tok in enumerate(line_srcs):\n","                    if tok == '▁':\n","                        word_idx+=1\n","                    elif tok[0] == '▁':\n","                        word_idx+=1\n","                        token2word_src[idx+1] = word_idx\n","                    else:\n","                        token2word_src[idx+1] = word_idx\n","                \n","                word_idx = 0\n","                token2word_tgt = {}\n","                for idx, tok in enumerate(line_tgts):\n","                    if tok == '▁':\n","                        word_idx+=1\n","                    elif tok[0] == '▁':\n","                        word_idx+=1\n","                        token2word_tgt[idx+1] = word_idx\n","                    else:\n","                        token2word_tgt[idx+1] = word_idx\n","\n","                word_aligns = []\n","                for align in line_in.split():\n","                    align_src = int(align.split('-')[0])\n","                    align_tgt = int(align.split('-')[1])\n","\n","                    if align_src in token2word_src and align_tgt in token2word_tgt:\n","                        align_word = str(token2word_src[align_src])+'-'+str(token2word_tgt[align_tgt])\n","                        if align_word not in word_aligns:\n","                            word_aligns.append(align_word)\n","\n","                file_out.write(\" \".join(word_aligns)+'\\n')\n","    \n","    return align_file+'.2word'\n","\n","def get_test_tokenized(file_name, num_sent_test):\n","    dir_out = \"/\".join(file_name.split('/')[:-1]) + '/' + file_name.split('/')[-1].replace('_train', '')\n","    if not os.path.isfile(dir_out):\n","        print('Create ', dir_out)\n","        with open(file_name, 'r') as file_in, open(dir_out, 'w+') as file_out:\n","            idx = 0\n","            for line_in in file_in:\n","                file_out.write(line_in)\n","                idx+=1\n","                if idx == num_sent_test: break\n","\n","        #print('Delete ', file_name)\n","        #open(file_name, 'w').close()\n","        #os.remove(file_name)\n","\n","    return dir_out\n","            \n","def train_fastalign(vocab_pair, file_src, file_tgt, num_sent_test, timeout_seconds=10):\n","    both = file_src.split('/')[-1] + '-' + file_tgt.split('/')[-1]\n","    both_dir = \"/\".join(file_src.split('/')[:-1]) + '/' + both\n","\n","    if not os.path.isfile(both_dir + '.align.shift1.2word'):\n","\n","        if not os.path.isfile(both_dir):\n","            print(vocab_pair, ': Create ', both_dir)\n","            with open(file_src, 'r') as file_src_, open(file_tgt, 'r') as file_tgt_, \\\n","            open(both_dir, \"w+\") as file_both:\n","                for line_src, line_tgt in zip(file_src_, file_tgt_):\n","                    if line_src != '\\n' and line_tgt != '\\n' and line_src != '' and line_tgt != '':\n","                        file_both.write(line_src.rstrip('\\n') + ' ||| ' + line_tgt)\n","            \n","        if not os.path.isfile(both_dir+'.align') and not os.path.isfile(both_dir+'.align.shift1'):\n","            cmd_fastalign = \"/content/fastalign/fast_align -i %s -d -o -v > %s.align\"%(both_dir, both_dir)\n","            os.system(cmd_fastalign)\n","            print(vocab_pair, ': Done training ', both_dir+'.align')\n","            wait(lambda: os.path.isfile(both_dir+'.align'), timeout_seconds=timeout_seconds, waiting_for=\"Fastalign returns alignment file\")\n","        \n","\n","        if not os.path.isfile(both_dir+'.align.shift1'):\n","            print(vocab_pair, ': Shift 1 ', both_dir+'.align.shift1')\n","            with open(both_dir+'.align', 'r') as file_in, open(both_dir+'.align.shift1', 'w+') as file_out:\n","                idx = 0\n","                for line_in in file_in:\n","                    aligns=[]\n","                    for align in line_in.split():\n","                        aligns.append(\"%d-%d\"%(int(align.split('-')[0])+1, int(align.split('-')[1])+1))\n","\n","                    line_out = \" \".join(aligns)\n","                    file_out.write(line_out+'\\n')\n","                    idx+=1\n","                    if idx == num_sent_test: break\n","        \n","            wait(lambda: os.path.isfile(both_dir+'.align.shift1'), timeout_seconds=timeout_seconds, waiting_for=\"Shift 1 for alignment file\")\n","\n","        if not os.path.isfile(both_dir+'.align.shift1.2word'):\n","            print(vocab_pair, ': Create ', both_dir+'.align.shift1.2word')\n","            is_word = [True if vocab=='word' else False for vocab in vocab_pair.split('-')]\n","            align_name = both_dir+'.align.shift1'\n","            src_name = \"/\".join(align_name.split('/')[:-1]) + '/' + align_name.split('/')[-1].split('.')[0].split('-')[0]\n","            tgt_name = \"/\".join(align_name.split('/')[:-1]) + '/' + align_name.split('/')[-1].split('.')[0].split('-')[1]\n","            \n","            final_output = token_2_word_alignment(src_name, tgt_name, align_name, is_word=is_word)\n","\n","            wait(lambda: os.path.isfile(final_output), timeout_seconds=timeout_seconds, waiting_for=\"Token_2_word for alignment file\")\n","\n","        print('-----', vocab_pair, ': DONE ', both_dir+'.align.shift1.2word')\n","    else:\n","        print('-----', vocab_pair, ': Already found')\n","\n","    if os.path.isfile(both_dir) and os.path.isfile(both_dir+'.align'):\n","        #print(vocab_pair, ': Delete ', both_dir)\n","        with open(both_dir+'.align', 'r') as f:\n","            count = 0\n","            for count, line in enumerate(f):\n","                pass\n","            num_line =  count + 1\n","        if num_line > 10:\n","            open(both_dir, 'w').close()\n","            os.remove(both_dir)\n","\n","    if os.path.isfile(both_dir+'.align') and os.path.isfile(both_dir+'.align.shift1'):\n","        #print(vocab_pair, ': Delete ', both_dir+'.align')\n","        with open(both_dir+'.align.shift1', 'r') as f:\n","            count = 0\n","            for count, line in enumerate(f):\n","                pass\n","            num_line =  count + 1\n","        if num_line > 10:\n","            open(both_dir+'.align', 'w').close()\n","            os.remove(both_dir+'.align')\n","\n","    if os.path.isfile(both_dir+'.align.shift1') and os.path.isfile(both_dir+'.align.shift1.2word'):\n","        #print(vocab_pair, ': Delete ', both_dir+'.align.shift1')\n","        with open(both_dir+'.align.shift1.2word', 'r') as f:\n","            count = 0\n","            for count, line in enumerate(f):\n","                pass\n","            num_line =  count + 1\n","        if num_line > 10:\n","            open(both_dir+'.align.shift1', 'w').close()\n","            os.remove(both_dir+'.align.shift1')\n","\n","    return both_dir+'.align.shift1.2word'\n","\n","def get_alignment_files(train_src, train_tgt, \n","                        test_src, test_tgt,\n","                        data_dir,\n","                        src,\n","                        tgt,\n","                        model_type='bpe',\n","                        vocab_sizes=None,\n","                        timeout_seconds=180):\n","    \n","    # Concatenate test and train set\n","    cmd_format = \"cat %s %s > %s%s_test_train\"\n","    cmd_src = cmd_format%(test_src, train_src, data_dir, src)\n","    cmd_tgt = cmd_format%(test_tgt, train_tgt, data_dir, tgt)\n","\n","    os.system('mkdir ' + data_dir)\n","\n","    print(cmd_src)\n","    print(cmd_tgt)\n","\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n","        executor.submit(os.system(cmd_src))\n","        executor.submit(os.system(cmd_tgt))\n","\n","    wait(lambda: os.path.isfile(data_dir + src + '_test_train'), timeout_seconds=timeout_seconds, waiting_for=\"Os creates source file\")\n","    wait(lambda: os.path.isfile(data_dir + tgt + '_test_train'), timeout_seconds=timeout_seconds, waiting_for=\"Os creates target file\")\n","\n","    #---------------------------------------------------------------------------\n","    # Get vocabulary size and create vocabulary file.\n","    start_time = time.time()\n","    \n","    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n","        vocab_word_size_src, vocab_char_size_src = executor.submit(get_dictionary, data_dir + src + '_test_train').result()\n","        vocab_word_size_tgt, vocab_char_size_tgt = executor.submit(get_dictionary, data_dir + tgt + '_test_train').result()\n","\n","    print(\"Source vocab:\", vocab_word_size_src, vocab_char_size_src)\n","    print(\"Target vocab:\", vocab_word_size_tgt, vocab_char_size_tgt)\n","\n","    if vocab_sizes is not None:\n","        vocab_src_sizes = [size for size in vocab_sizes if size > vocab_char_size_src and size < vocab_word_size_src]\n","        vocab_tgt_sizes = [size for size in vocab_sizes if size > vocab_char_size_tgt and size < vocab_word_size_tgt]\n","\n","        print(\"Search space:\", vocab_sizes)\n","        print(\"Search space for Source:\", vocab_src_sizes)\n","        print(\"Search space for Target:\", vocab_tgt_sizes)\n","\n","    print(\"Preparing vocabulary finised in %f\"%(time.time()-start_time))\n","\n","    #---------------------------------------------------------------------------\n","    start_time = time.time()\n","    files_src = {}\n","    files_tgt = {}\n","\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n","        output_src = executor.submit(get_tokenized_corpus, data_dir + src + '_test_train', 'char')\n","        output_tgt = executor.submit(get_tokenized_corpus, data_dir + tgt + '_test_train', 'char')\n","\n","        token_char_src, size_src = output_src.result()\n","        token_char_tgt, size_tgt = output_tgt.result()\n","\n","    files_src['char'] = token_char_src\n","    files_tgt['char'] = token_char_tgt\n","\n","    if vocab_sizes is not None:\n","        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n","            output_src = [executor.submit(get_tokenized_corpus, data_dir + src + '_test_train', model_type, vocab_src) for vocab_src in vocab_src_sizes] \n","            output_tgt = [executor.submit(get_tokenized_corpus, data_dir + tgt + '_test_train', model_type, vocab_tgt) for vocab_tgt in vocab_tgt_sizes] \n","            \n","            for output in concurrent.futures.as_completed(output_src):\n","                files_src[str(output.result()[1])] = output.result()[0]\n","\n","            for output in concurrent.futures.as_completed(output_tgt):\n","                files_tgt[str(output.result()[1])] = output.result()[0]\n","\n","    files_src['word'] = data_dir + src + '_test_train'\n","    files_tgt['word'] = data_dir + tgt + '_test_train'\n","    \n","    print(\"Tokenization files:\")\n","    print(files_src)\n","    print(files_tgt)\n","    print(\"Tokenization finised in %f\"%(time.time()-start_time))\n","\n","    #---------------------------------------------------------------------------\n","    start_time = time.time()\n","\n","    with open(test_src, 'r') as test_src_file:\n","        for count, line in enumerate(test_src_file):\n","            pass\n","        num_sent_test =  count + 1\n","\n","    files_align2word_src_tgt = {}\n","    files_align2word_tgt_src = {}\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n","        outputs_src_tgt = {}\n","        outputs_tgt_src = {}\n","\n","        for src_ in files_src:\n","            for tgt_ in files_tgt:\n","                outputs_src_tgt[src_+'-'+tgt_] = executor.submit(train_fastalign, src_+'-'+tgt_, files_src[src_], files_tgt[tgt_], num_sent_test, timeout_seconds)\n","\n","        for tgt_ in files_tgt:\n","            for src_ in files_src:\n","                outputs_tgt_src[tgt_+'-'+src_] = executor.submit(train_fastalign, tgt_+'-'+src_, files_tgt[tgt_], files_src[src_], num_sent_test, timeout_seconds)\n","\n","        for vocab_pair in outputs_src_tgt:\n","            files_align2word_src_tgt[vocab_pair] = outputs_src_tgt[vocab_pair].result()\n","\n","        for vocab_pair in outputs_tgt_src:\n","            files_align2word_tgt_src[vocab_pair] = outputs_tgt_src[vocab_pair].result()\n","\n","    print(\"Alignment-word output files:\")\n","    print(\"Src-Tgt:\", len(files_align2word_src_tgt), files_align2word_src_tgt)\n","    print(\"Tgt-Src:\", len(files_align2word_tgt_src), files_align2word_tgt_src)\n","\n","    with open(data_dir+\"output_%s_%s\"%(src,tgt), 'w+') as f: \n","        for key, value in files_align2word_src_tgt.items(): \n","            f.write('%s %s\\n' % (key, value))\n","\n","    with open(data_dir+\"output_%s_%s\"%(tgt,src), 'w+') as f: \n","        for key, value in files_align2word_tgt_src.items(): \n","            f.write('%s %s\\n' % (key, value))\n","\n","    files_test_src = {}\n","    files_test_tgt = {}\n","\n","    for vocab in files_src:\n","        dir_out = get_test_tokenized(files_src[vocab], num_sent_test)\n","        files_test_src[vocab] = dir_out\n","\n","    for vocab in files_tgt:\n","        dir_out = get_test_tokenized(files_tgt[vocab], num_sent_test)\n","        files_test_tgt[vocab] = dir_out\n","\n","    with open(data_dir+\"output_%s\"%(src), 'w+') as f: \n","            for key, value in files_test_src.items(): \n","                f.write('%s %s\\n' % (key, value))\n","\n","    with open(data_dir+\"output_%s\"%(tgt), 'w+') as f: \n","        for key, value in files_test_tgt.items(): \n","            f.write('%s %s\\n' % (key, value))\n","\n","    print(\"Training and Re-aligning finised in %f\"%(time.time()-start_time))"]},{"cell_type":"code","source":["import numpy as np\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","import matplotlib.colors as mcolors\n","import json\n","import concurrent.futures\n","\n","def clean_alignment(line):\n","    line = line.replace(\"\\n\",'')\n","    return line\n","\n","def clean_sentence(line):\n","    line = line.replace(\"\\n\",'')\n","    return line\n","\n","def get_prediction(prediction_file_name):\n","    prediction_file = open(prediction_file_name, 'r', encoding='utf-8')\n","    prediction_output = []\n","    for line in prediction_file:\n","        prediction_output.append([ x for x in clean_alignment(line).split() ])\n","        \n","    return prediction_output\n","\n","    \n","def get_reference(reference_file_name, ref_only=True):\n","    reference_file = open(reference_file_name, 'r', encoding='utf-8')\n","    \n","    reference_lines = reference_file.readlines()\n","    reference_lines = np.reshape(reference_lines, (int(len(reference_lines)/2), 2))\n","\n","    ref = []\n","    \n","    if ref_only:\n","        for s, p in zip(reference_lines[:,0], reference_lines[:,1]):\n","            s = clean_alignment(s)\n","            p = clean_alignment(p)\n","            ref.append(s.split() + p.split())\n","        return ref\n","    else:\n","        sure = []\n","        possible = []\n","    \n","        for s, p in zip(reference_lines[:,0], reference_lines[:,1]):\n","            s = clean_alignment(s)\n","            p = clean_alignment(p)\n","            sure.append(s.split())\n","            possible.append(p.split())\n","            ref.append(s.split() + p.split())\n","        \n","        return ref, sure, possible\n","        \n","def get_corpus_file(corpus_file_name):\n","    corpus_file = open(corpus_file_name, 'r', encoding='utf-8')\n","    corpus_output = []\n","    for line in corpus_file:\n","        corpus_output.append([ x for x in clean_sentence(line).split() ])\n","        \n","    return corpus_output\n","\n","def calculate_AER(reference_file_name, prediction_file_name):\n","    reference_set, sure_set, fuzzy_set = get_reference(reference_file_name, ref_only=False)\n","    prediction_set = get_prediction(prediction_file_name)\n","    \n","    sure_correct = 0.\n","    fuzzy_correct = 0.\n","    count_alignment = 0.\n","    count_sure = 0.\n","    \n","    for sure, fuzzy, prediction in zip(sure_set, fuzzy_set, prediction_set):\n","        for align in prediction:\n","            if align in sure:\n","                sure_correct+=1.\n","            if align in fuzzy:\n","                fuzzy_correct+=1.\n","                \n","        count_alignment += float(len(prediction))\n","        count_sure += float(len(sure))\n","    \n","    aer = 1. - (sure_correct*2 + fuzzy_correct)/ (count_alignment + count_sure)\n","    \n","    return aer\n","\n","def calculate_scores(tp, fp, tn, fn):\n","    acc = 0.\n","    if tp + fp + tn + fn != 0:\n","        acc = (tp+tn)/(tp + fp + tn + fn)\n","        \n","    precision = 0.\n","    if tp+fp != 0.:\n","        precision = tp/(tp+fp)\n","        \n","    recall = 0.\n","    if tp+fn != 0.:\n","        recall = tp/(tp+fn)\n","        \n","    f1 = 0.\n","    if precision+recall != 0:\n","        f1 = (2*precision*recall)/(precision+recall)\n","    \n","    return acc, precision, recall, f1\n","\n","def analyse_reference(reference_set,\n","                      sure_set, possible_set,\n","                      source_set, target_set):\n","\n","    #-------------------------------------\n","    total_num_link = 0\n","    total_null_link = 0\n","    \n","    num_word_source = 0\n","    num_word_target = 0\n","\n","    len_word_source = []\n","    len_word_target = []\n","\n","    len_char_source = []\n","    len_char_target = []\n","    \n","    # Count for Reference\n","    num_align_ref = 0\n","    num_no_ref = 0\n","    \n","    num_sure = 0\n","    num_fuzzy = 0\n","    \n","    num_align_ref_one2one = 0\n","    num_align_ref_one2many_source = 0\n","    num_align_ref_one2many_target = 0\n","    num_align_ref_many2one_source = 0\n","    num_align_ref_many2one_target = 0\n","    num_align_ref_many2many = 0\n","    num_align_ref_many2many_source = 0\n","    num_align_ref_many2many_target = 0\n","    \n","    num_no_ref_no = 0\n","    num_no_ref_null = 0\n","\n","\n","    # Null\n","    num_source2null_ref = 0\n","    num_target2null_ref = 0\n","    num_source2notnull_ref = 0\n","    num_target2notnull_ref = 0\n","    \n","    num_source2null_ref_ratio_list = []\n","    num_target2null_ref_ratio_list = []\n","    \n","    for line in sure_set:\n","        for s in line:\n","            num_sure+= 1\n","    \n","    for line in possible_set:\n","        for p in line:\n","            num_fuzzy+= 1\n","    \n","    for idx, [source, target, ref] in enumerate(zip(source_set, target_set, reference_set)):\n","        \n","        source_len = len(source)\n","        target_len = len(target)\n","\n","        len_word_source.append(source_len)\n","        len_word_target.append(target_len)\n","\n","        len_char_source_ = 0\n","        for word in source:\n","            len_char_source_+=len(word)\n","        len_char_source.append(len_char_source_)\n","\n","        len_char_target_ = 0\n","        for word in target:\n","            len_char_target_+=len(word)\n","        len_char_target.append(len_char_target_)\n","\n","        \n","        total_num_link += ((source_len) * (target_len))\n","        total_null_link += ((source_len) + (target_len))\n","        \n","        num_word_source += source_len\n","        num_word_target += target_len\n","        \n","\n","        #---------------------------\n","        # Null\n","        num_source2null_ref_sent = 0\n","        num_target2null_ref_sent = 0\n","        \n","        for idx_source in range(1, source_len+1):\n","            source_to_null_ref = True\n","            for idx_target in range(1, target_len+1):\n","                align_check = str(idx_source) +'-'+ str(idx_target)\n","                if align_check in ref:\n","                    source_to_null_ref = False\n","                    \n","            if source_to_null_ref:\n","                num_source2null_ref+=1\n","                num_source2null_ref_sent+=1\n","            else:\n","                num_source2notnull_ref+=1\n","                    \n","            \n","        for idx_target in range(1, target_len+1):\n","            target_to_null_ref = True\n","            for idx_source in range(1, source_len+1):\n","                align_check = str(idx_source) +'-'+ str(idx_target)\n","                if align_check in ref:\n","                    target_to_null_ref = False\n","                    \n","            if target_to_null_ref:\n","                num_target2null_ref+=1\n","                num_target2null_ref_sent+=1\n","            else:\n","                num_target2notnull_ref+=1\n","                \n","\n","                \n","        num_source2null_ref_ratio_list.append(num_source2null_ref_sent/source_len)\n","        num_target2null_ref_ratio_list.append(num_target2null_ref_sent/target_len)\n","        \n","        num_align_ref_one2many_source_list = []\n","        num_align_ref_many2one_target_list = []\n","        num_align_ref_many2many_source_list = []\n","        num_align_ref_many2many_target_list = []\n","        \n","        align_ref_one2one_list = []\n","        align_ref_one2many_list = []\n","        align_ref_many2one_list = []\n","        align_ref_many2many_list = []\n","        \n","        for idx_source in range(1, source_len+1):\n","            for idx_target in range(1, target_len+1):\n","                align_check = str(idx_source) +'-'+ str(idx_target)\n","                \n","                # Count number of links in Ref\n","                if align_check in ref:\n","                    num_align_ref +=1\n","                    \n","                    check_one2many = False\n","                    check_many2one = False\n","                    for idx_target_ in range(1, target_len+1):\n","                        align_check_ = str(idx_source) +'-'+ str(idx_target_)\n","                        if align_check_ in ref and align_check_ != align_check:\n","                            check_one2many = True\n","                    for idx_source_ in range(1, source_len+1):\n","                        align_check_ = str(idx_source_) +'-'+ str(idx_target)\n","                        if align_check_ in ref and align_check_ != align_check:\n","                            check_many2one = True\n","                    \n","                    if check_one2many is True and check_many2one is False:\n","                        num_align_ref_one2many_target +=1\n","                        align_ref_one2many_list.append(align_check)\n","                        if idx_source not in num_align_ref_one2many_source_list:\n","                            num_align_ref_one2many_source_list.append(idx_source)\n","                    if check_many2one is True and check_one2many is False:\n","                        num_align_ref_many2one_source +=1\n","                        align_ref_many2one_list.append(align_check)\n","                        if idx_target not in num_align_ref_many2one_target_list:\n","                            num_align_ref_many2one_target_list.append(idx_target)\n","                    if check_one2many is False and check_many2one is False:\n","                        num_align_ref_one2one +=1\n","                        align_ref_one2one_list.append(align_check)\n","                        \n","                # Count number of links not in Ref\n","                if align_check not in ref:\n","                    num_no_ref +=1\n","                    \n","                    source_to_null = True\n","                    for idx_target_ in range(1, target_len+1):\n","                        align_check_ = str(idx_source) +'-'+ str(idx_target_)\n","                        if align_check_ in ref:\n","                            source_to_null = False\n","                    \n","                    target_to_null = True\n","                    for idx_source_ in range(1, source_len+1):\n","                        align_check_ = str(idx_source_) +'-'+ str(idx_target)\n","                        if align_check_ in ref:\n","                            target_to_null = False\n","                            \n","                    if source_to_null and target_to_null:\n","                        num_no_ref_null +=1\n","                        \n","                    else:\n","                        num_no_ref_no +=1\n","                \n","                        \n","        for idx_source in range(1, source_len+1):\n","            for idx_target in range(1, target_len+1):\n","                align_check = str(idx_source) +'-'+ str(idx_target)\n","                # Count number of many2many links in Ref\n","                if align_check in ref:\n","                    \n","                    check_one2many = False\n","                    check_many2one = False\n","                    for idx_target_ in range(1, target_len+1):\n","                        align_check_ = str(idx_source) +'-'+ str(idx_target_)\n","                        if align_check_ in ref and align_check_ != align_check:\n","                            check_one2many = True\n","                    for idx_source_ in range(1, source_len+1):\n","                        align_check_ = str(idx_source_) +'-'+ str(idx_target)\n","                        if align_check_ in ref and align_check_ != align_check:\n","                            check_many2one = True\n","                            \n","                    if check_one2many is True and check_many2one is True:\n","                        if idx_source not in num_align_ref_many2many_source_list \\\n","                        and idx_source not in num_align_ref_one2many_source_list:\n","                            num_align_ref_many2many_source_list.append(idx_source)\n","                        if idx_target not in num_align_ref_many2many_target_list \\\n","                        and idx_target not in num_align_ref_many2one_target_list:\n","                            num_align_ref_many2many_target_list.append(idx_target)\n","                        num_align_ref_many2many+=1\n","                        align_ref_many2many_list.append(align_check)\n","                        \n","                        \n","        num_align_ref_one2many_source += len(num_align_ref_one2many_source_list)\n","        num_align_ref_many2one_target += len(num_align_ref_many2one_target_list)\n","        \n","        num_align_ref_many2many_source += len(num_align_ref_many2many_source_list)\n","        num_align_ref_many2many_target += len(num_align_ref_many2many_target_list)\n","    \n","    \n","    num_source2null_ref_ratio_mean = np.mean(num_source2null_ref_ratio_list)\n","    num_target2null_ref_ratio_mean = np.mean(num_target2null_ref_ratio_list)\n","    \n","    \n","    #-------------------------------------\n","    values = {\"num_word_source\": num_word_source, \n","              \"num_word_target\": num_word_target,\n","              \"total_num_link\": total_num_link,\n","              \"num_sure\": num_sure, \n","              \"num_fuzzy\": num_fuzzy,\n","              \n","              \"num_align_ref\": num_align_ref, \n","              \"num_no_ref\": num_no_ref, \n","              \"num_no_ref_no\": num_no_ref_no, \n","              \"num_no_ref_null\": num_no_ref_null,\n","\n","              \"num_align_ref_one2one\": num_align_ref_one2one, \n","              \"num_align_ref_one2many_source\": num_align_ref_one2many_source, \n","              \"num_align_ref_one2many_target\": num_align_ref_one2many_target,\n","              \"num_align_ref_many2one_source\": num_align_ref_many2one_source, \n","              \"num_align_ref_many2one_target\": num_align_ref_many2one_target,\n","              \"num_align_ref_many2many\": num_align_ref_many2many, \n","              \"num_align_ref_many2many_source\": num_align_ref_many2many_source, \n","              \"num_align_ref_many2many_target\": num_align_ref_many2many_target,\n","\n","              \"total_null_link\": total_null_link, \n","              \"num_source2null_ref\": num_source2null_ref, \n","              \"num_target2null_ref\": num_target2null_ref, \n","              \"num_source2notnull_ref\": num_source2notnull_ref, \n","              \"num_target2notnull_ref\": num_target2notnull_ref,\n","              \"num_source2null_ref_ratio_mean\": num_source2null_ref_ratio_mean, \n","              \"num_target2null_ref_ratio_mean\": num_target2null_ref_ratio_mean,\n","              \n","              \"len_word_source\": len_word_source,\n","              \"len_word_target\": len_word_target,\n","              \n","              \"len_char_source\": len_char_source,\n","              \"len_char_target\": len_char_target\n","\n","              }\n","    \n","    return values\n","\n","def analyse_prediction(prediction_set,\n","                       reference_set,\n","                       sure_set, fuzzy_set,\n","                       source_set,\n","                       target_set):\n","\n","    # Count for Prediction\n","    num_align_pred = 0\n","    num_no_pred = 0\n","    num_align_pred_one2one = 0\n","    num_align_pred_one2many_source = 0\n","    num_align_pred_one2many_target = 0\n","    num_align_pred_many2one_source = 0\n","    num_align_pred_many2one_target = 0\n","    num_align_pred_many2many = 0\n","    num_align_pred_many2many_source = 0\n","    num_align_pred_many2many_target = 0\n","\n","    num_no_pred_no = 0\n","    num_no_pred_null = 0\n","    \n","    #TP\n","    num_true_align_tp = 0\n","    \n","    num_true_align_tp_one2one_pred = 0\n","    num_true_align_tp_one2many_pred = 0\n","    num_true_align_tp_many2one_pred = 0\n","    num_true_align_tp_many2many_pred = 0\n","    \n","    #TN\n","    num_true_no_tn = 0\n","    num_true_no_tn_no_in_pred = 0\n","    num_true_no_tn_null_in_pred = 0\n","    \n","    #FN\n","    num_false_no_fn = 0\n","    num_false_no_fn_no_in_pred = 0\n","    num_false_no_fn_null_in_pred = 0\n","    \n","    #FP\n","    num_false_align_no_fp = 0\n","    num_false_align_no_fp_one2one_pred = 0\n","    num_false_align_no_fp_one2many_pred = 0\n","    num_false_align_no_fp_many2one_pred = 0\n","    num_false_align_no_fp_many2many_pred = 0\n","\n","    num_false_align_no_fp_no_in_ref = 0\n","    num_false_align_no_fp_null_in_ref = 0\n","\n","    # Null\n","    num_source2null_pred = 0\n","    num_target2null_pred = 0\n","    num_source2notnull_pred = 0\n","    num_target2notnull_pred = 0\n","    \n","    num_source2null_pred_tp = 0\n","    num_source2null_pred_fp = 0\n","    num_source2null_pred_tn = 0\n","    num_source2null_pred_fn = 0\n","    \n","    num_target2null_pred_tp = 0\n","    num_target2null_pred_fp = 0\n","    num_target2null_pred_tn = 0\n","    num_target2null_pred_fn = 0\n","    \n","    num_source2null_pred_ratio_list = []\n","    num_target2null_pred_ratio_list = []\n","    \n","    num_true_null_tp = 0\n","    num_false_not_null_fp = 0\n","    num_false_null_fn = 0\n","    num_true_not_null_tn = 0\n","    \n","    for idx, [source, target, pred, ref] in enumerate(zip(source_set, target_set,\n","                                                                      prediction_set, reference_set)):\n","        \n","        source_len = len(source)\n","        target_len = len(target)\n","\n","        #---------------------------\n","        # Null\n","        num_source2null_pred_sent = 0\n","        num_target2null_pred_sent = 0\n","        \n","        for idx_source in range(1, source_len+1):\n","            source_to_null_ref = True\n","            source_to_null_pred = True\n","            \n","            for idx_target in range(1, target_len+1):\n","                align_check = str(idx_source) +'-'+ str(idx_target)\n","                if align_check in ref:\n","                    source_to_null_ref = False\n","                if align_check in pred:\n","                    source_to_null_pred = False\n","                    \n","                \n","            if source_to_null_pred:\n","                num_source2null_pred+=1\n","                num_source2null_pred_sent+=1\n","                \n","            else:\n","                num_source2notnull_pred+=1\n","                \n","            if source_to_null_ref and source_to_null_pred:\n","                num_true_null_tp+=1\n","                num_source2null_pred_tp+=1\n","                \n","            if not source_to_null_ref and not source_to_null_pred:\n","                num_true_not_null_tn+=1  \n","                num_source2null_pred_tn+=1\n","                \n","            if source_to_null_ref and not source_to_null_pred:\n","                num_false_null_fn+=1\n","                num_source2null_pred_fn+=1\n","                \n","            if not source_to_null_ref and source_to_null_pred:\n","                num_false_not_null_fp+=1\n","                num_source2null_pred_fp+=1\n","                \n","            \n","        for idx_target in range(1, target_len+1):\n","            target_to_null_ref = True\n","            target_to_null_pred = True\n","            \n","            for idx_source in range(1, source_len+1):\n","                align_check = str(idx_source) +'-'+ str(idx_target)\n","                if align_check in ref:\n","                    target_to_null_ref = False\n","                if align_check in pred:\n","                    target_to_null_pred = False\n","                \n","            if target_to_null_pred:\n","                num_target2null_pred+=1\n","                num_target2null_pred_sent+=1\n","            else:\n","                num_target2notnull_pred+=1\n","                \n","            if target_to_null_ref and target_to_null_pred:\n","                num_true_null_tp+=1\n","                num_target2null_pred_tp+=1\n","            if not target_to_null_ref and not target_to_null_pred:\n","                num_true_not_null_tn+=1  \n","                num_target2null_pred_tn+=1\n","            if target_to_null_ref and not target_to_null_pred:\n","                num_false_null_fn+=1\n","                num_target2null_pred_fn+=1\n","            if not target_to_null_ref and target_to_null_pred:\n","                num_false_not_null_fp+=1\n","                num_target2null_pred_fp+=1\n","                \n","                \n","        num_source2null_pred_ratio_list.append(num_source2null_pred_sent/source_len)\n","        num_target2null_pred_ratio_list.append(num_target2null_pred_sent/target_len)\n","        \n","        \n","        num_align_pred_one2many_source_list = []\n","        num_align_pred_many2one_target_list = []\n","        num_align_pred_many2many_source_list = []\n","        num_align_pred_many2many_target_list = []\n","        \n","        align_pred_one2one_list = []\n","        align_pred_one2many_list = []\n","        align_pred_many2one_list = []\n","        align_pred_many2many_list = []\n","        \n","        for idx_source in range(1, source_len+1):\n","            for idx_target in range(1, target_len+1):\n","                align_check = str(idx_source) +'-'+ str(idx_target)\n","\n","                # Count number of links in Prediction\n","                if align_check in pred:\n","                    num_align_pred +=1\n","                    \n","                    check_one2many = False\n","                    check_many2one = False\n","                    for idx_target_ in range(1, target_len+1):\n","                        align_check_ = str(idx_source) +'-'+ str(idx_target_)\n","                        if align_check_ in pred and align_check_ != align_check:\n","                            check_one2many = True\n","                    for idx_source_ in range(1, source_len+1):\n","                        align_check_ = str(idx_source_) +'-'+ str(idx_target)\n","                        if align_check_ in pred and align_check_ != align_check:\n","                            check_many2one = True\n","                    \n","                    if check_one2many is True and check_many2one is False:\n","                        num_align_pred_one2many_target +=1\n","                        align_pred_one2many_list.append(align_check)\n","                        if idx_source not in num_align_pred_one2many_source_list:\n","                            num_align_pred_one2many_source_list.append(idx_source)\n","                    if check_many2one is True and check_one2many is False:\n","                        num_align_pred_many2one_source +=1\n","                        align_pred_many2one_list.append(align_check)\n","                        if idx_target not in num_align_pred_many2one_target_list:\n","                            num_align_pred_many2one_target_list.append(idx_target)\n","                    if check_one2many is False and check_many2one is False:\n","                        num_align_pred_one2one +=1\n","                        align_pred_one2one_list.append(align_check)\n","                \n","                # Count number of links not in Prediction\n","                if align_check not in pred:\n","                    num_no_pred+=1\n","                    \n","                    source_to_null = True\n","                    for idx_target_ in range(1, target_len+1):\n","                        align_check_ = str(idx_source) +'-'+ str(idx_target_)\n","                        if align_check_ in pred:\n","                            source_to_null = False\n","                    \n","                    target_to_null = True\n","                    for idx_source_ in range(1, source_len+1):\n","                        align_check_ = str(idx_source_) +'-'+ str(idx_target)\n","                        if align_check_ in pred:\n","                            target_to_null = False\n","                            \n","                    if source_to_null and target_to_null:\n","                        num_no_pred_null +=1\n","                    else:\n","                        num_no_pred_no +=1\n","                        \n","        for idx_source in range(1, source_len+1):\n","            for idx_target in range(1, target_len+1):\n","                align_check = str(idx_source) +'-'+ str(idx_target)\n","                \n","                # Count number of many2many links in Prediction\n","                if align_check in pred:\n","                    \n","                    check_one2many = False\n","                    check_many2one = False\n","                    for idx_target_ in range(1, target_len+1):\n","                        align_check_ = str(idx_source) +'-'+ str(idx_target_)\n","                        if align_check_ in pred and align_check_ != align_check:\n","                            check_one2many = True\n","                    for idx_source_ in range(1, source_len+1):\n","                        align_check_ = str(idx_source_) +'-'+ str(idx_target)\n","                        if align_check_ in pred and align_check_ != align_check:\n","                            check_many2one = True\n","                            \n","                    if check_one2many is True and check_many2one is True:\n","                        if idx_source not in num_align_pred_many2many_source_list \\\n","                        and idx_source not in num_align_pred_one2many_source_list:\n","                            num_align_pred_many2many_source_list.append(idx_source)\n","                        if idx_target not in num_align_pred_many2many_target_list \\\n","                        and idx_target not in num_align_pred_many2one_target_list:\n","                            num_align_pred_many2many_target_list.append(idx_target)\n","                        num_align_pred_many2many+=1\n","                        align_pred_many2many_list.append(align_check)\n","                        \n","        for idx_source in range(1, source_len+1):\n","            for idx_target in range(1, target_len+1):\n","                align_check = str(idx_source) +'-'+ str(idx_target)\n","                \n","                    \n","                # Count number of links in Prediction and in Ref: TP\n","                if align_check in pred and align_check in ref:\n","                    num_true_align_tp +=1\n","                    \n","                    if align_check in align_pred_one2one_list:\n","                        num_true_align_tp_one2one_pred+=1\n","                    if align_check in align_pred_one2many_list:\n","                        num_true_align_tp_one2many_pred+=1\n","                    if align_check in align_pred_many2one_list:\n","                        num_true_align_tp_many2one_pred+=1\n","                    if align_check in align_pred_many2many_list:\n","                        num_true_align_tp_many2many_pred+=1\n","                \n","                        \n","                # Count number of links not in Prediction and not in Ref: TN\n","                if align_check not in pred and align_check not in ref:\n","                    num_true_no_tn+=1\n","                    \n","                    source_to_null = True\n","                    for idx_target_ in range(1, target_len+1):\n","                        align_check_ = str(idx_source) +'-'+ str(idx_target_)\n","                        if align_check_ in pred:\n","                            source_to_null = False\n","                    \n","                    target_to_null = True\n","                    for idx_source_ in range(1, source_len+1):\n","                        align_check_ = str(idx_source_) +'-'+ str(idx_target)\n","                        if align_check_ in pred:\n","                            target_to_null = False\n","                            \n","                    if source_to_null and target_to_null:\n","                        num_true_no_tn_null_in_pred +=1\n","                    else:\n","                        num_true_no_tn_no_in_pred +=1\n","                    \n","                # Count number of links in Prediction and not in Ref: FP\n","                if align_check in pred and align_check not in ref:\n","                    num_false_align_no_fp+=1\n","                    \n","                    if align_check in align_pred_one2one_list:\n","                        num_false_align_no_fp_one2one_pred+=1\n","                    if align_check in align_pred_one2many_list:\n","                        num_false_align_no_fp_one2many_pred+=1\n","                    if align_check in align_pred_many2one_list:\n","                        num_false_align_no_fp_many2one_pred+=1\n","                    if align_check in align_pred_many2many_list:\n","                        num_false_align_no_fp_many2many_pred+=1\n","                        \n","                    source_to_null = True\n","                    for idx_target_ in range(1, target_len+1):\n","                        align_check_ = str(idx_source) +'-'+ str(idx_target_)\n","                        if align_check_ in ref:\n","                            source_to_null = False\n","                    \n","                    target_to_null = True\n","                    for idx_source_ in range(1, source_len+1):\n","                        align_check_ = str(idx_source_) +'-'+ str(idx_target)\n","                        if align_check_ in ref:\n","                            target_to_null = False\n","                    \n","                    if source_to_null and target_to_null:\n","                        num_false_align_no_fp_null_in_ref +=1\n","                    else:\n","                        num_false_align_no_fp_no_in_ref +=1\n","                        \n","                # Count number of links not in Prediction and in Ref: FN\n","                if align_check not in pred and align_check in ref:\n","                    num_false_no_fn+=1\n","                    \n","                    source_to_null = True\n","                    for idx_target_ in range(1, target_len+1):\n","                        align_check_ = str(idx_source) +'-'+ str(idx_target_)\n","                        if align_check_ in pred:\n","                            source_to_null = False\n","                    \n","                    target_to_null = True\n","                    for idx_source_ in range(1, source_len+1):\n","                        align_check_ = str(idx_source_) +'-'+ str(idx_target)\n","                        if align_check_ in pred:\n","                            target_to_null = False\n","                    \n","                    if source_to_null and target_to_null:\n","                        num_false_no_fn_null_in_pred +=1\n","                    else:\n","                        num_false_no_fn_no_in_pred +=1  \n","        \n","        num_align_pred_one2many_source += len(num_align_pred_one2many_source_list)\n","        num_align_pred_many2one_target += len(num_align_pred_many2one_target_list)\n","        \n","        num_align_pred_many2many_source += len(num_align_pred_many2many_source_list)\n","        num_align_pred_many2many_target += len(num_align_pred_many2many_target_list)\n","    \n","    num_source2null_pred_ratio_mean = np.mean(num_source2null_pred_ratio_list)\n","    num_target2null_pred_ratio_mean = np.mean(num_target2null_pred_ratio_list)\n","        \n","    acc, precision, recall, f1 = calculate_scores(num_true_align_tp, num_false_align_no_fp, num_true_no_tn, num_false_no_fn)\n","    \n","    null_acc, null_precision, null_recall, null_f1 = calculate_scores(num_true_null_tp, num_false_not_null_fp, num_true_not_null_tn, num_false_null_fn)\n","\n","    sure_correct = 0.\n","    fuzzy_correct = 0.\n","    count_alignment = 0.\n","    count_sure = 0.\n","\n","    for sure, fuzzy, prediction in zip(sure_set, fuzzy_set, prediction_set):\n","        for align in prediction:\n","            if align in sure:\n","                sure_correct+=1.\n","            if align in fuzzy:\n","                fuzzy_correct+=1.\n","                \n","        count_alignment += float(len(prediction))\n","        count_sure += float(len(sure))\n","    \n","    aer = 1. - (sure_correct*2 + fuzzy_correct)/ (count_alignment + count_sure)\n","    \n","    values = {\"num_align_pred\": num_align_pred, \n","              \"num_no_pred\": num_no_pred, \n","              \"num_no_pred_no\": num_no_pred_no, \n","              \"num_no_pred_null\": num_no_pred_null,\n","              \n","              \"num_true_align_tp\": num_true_align_tp,\n","              \"num_false_align_no_fp\": num_false_align_no_fp,\n","              \"num_false_align_no_fp_no_in_ref\": num_false_align_no_fp_no_in_ref, \n","              \"num_false_align_no_fp_null_in_ref\": num_false_align_no_fp_null_in_ref,\n","              \"num_false_no_fn\": num_false_no_fn, \n","              \"num_false_no_fn_no_in_pred\": num_false_no_fn_no_in_pred, \n","              \"num_false_no_fn_null_in_pred\": num_false_no_fn_null_in_pred,\n","              \"num_true_no_tn\": num_true_no_tn, \n","              \"num_true_no_tn_no_in_pred\": num_true_no_tn_no_in_pred, \n","              \"num_true_no_tn_null_in_pred\": num_true_no_tn_null_in_pred,\n","              \n","              \"acc\": acc, \n","              \"precision\": precision, \n","              \"recall\": recall, \n","              \"f1\": f1,\n","              \"aer\": aer,\n","             \n","              \"num_align_pred_one2one\": num_align_pred_one2one,\n","              \"num_align_pred_one2many_source\": num_align_pred_one2many_source, \n","              \"num_align_pred_one2many_target\": num_align_pred_one2many_target,\n","              \"num_align_pred_many2one_source\": num_align_pred_many2one_source, \n","              \"num_align_pred_many2one_target\": num_align_pred_many2one_target,\n","              \"num_align_pred_many2many\": num_align_pred_many2many, \n","              \"num_align_pred_many2many_source\": num_align_pred_many2many_source, \n","              \"num_align_pred_many2many_target\": num_align_pred_many2many_target,\n","             \n","              \"num_true_align_tp_one2one_pred\": num_true_align_tp_one2one_pred, \n","              \"num_true_align_tp_one2many_pred\": num_true_align_tp_one2many_pred,\n","              \"num_true_align_tp_many2one_pred\": num_true_align_tp_many2one_pred, \n","              \"num_true_align_tp_many2many_pred\": num_true_align_tp_many2many_pred,\n","              \"num_false_align_no_fp_one2one_pred\": num_false_align_no_fp_one2one_pred, \n","              \"num_false_align_no_fp_one2many_pred\": num_false_align_no_fp_one2many_pred,\n","              \"num_false_align_no_fp_many2one_pred\": num_false_align_no_fp_many2one_pred, \n","              \"num_false_align_no_fp_many2many_pred\": num_false_align_no_fp_many2many_pred,\n","             \n","              \"num_source2null_pred\": num_source2null_pred, \n","              \"num_target2null_pred\": num_target2null_pred, \n","              \"num_source2notnull_pred\": num_source2notnull_pred, \n","              \"num_target2notnull_pred\": num_target2notnull_pred,\n","             \n","              \"num_source2null_pred_tp\": num_source2null_pred_tp, \n","              \"num_source2null_pred_fp\": num_source2null_pred_fp, \n","              \"num_source2null_pred_fn\": num_source2null_pred_fn, \n","              \"num_source2null_pred_tn\": num_source2null_pred_tn,\n","              \"num_target2null_pred_tp\": num_target2null_pred_tp,\n","              \"num_target2null_pred_fp\": num_target2null_pred_fp, \n","              \"num_target2null_pred_fn\": num_target2null_pred_fn, \n","              \"num_target2null_pred_tn\": num_target2null_pred_tn,\n","             \n","              \"num_source2null_pred_ratio_mean\": num_source2null_pred_ratio_mean, \n","              \"num_target2null_pred_ratio_mean\": num_target2null_pred_ratio_mean,\n","              \"num_true_null_tp\": num_true_null_tp, \n","              \"num_false_not_null_fp\": num_false_not_null_fp, \n","              \"num_false_null_fn\": num_false_null_fn, \n","              \"num_true_not_null_tn\": num_true_not_null_tn,\n","             \n","              \"null_acc\": null_acc, \n","              \"null_precision\": null_precision, \n","              \"null_recall\": null_recall, \n","              \"null_f1\": null_f1,\n","\n","              }\n","\n","    return values\n","\n","def get_len(file_name):\n","    lens_word = []\n","    lens_char = []\n","    with open(file_name, 'r') as f:\n","        for line in f:\n","            words = line.split()\n","            lens_word.append(len(words))\n","\n","            len_char = 0\n","            for w in words:\n","                len_char+= len(w)\n","            \n","            lens_char.append(len_char)\n","\n","    return lens_word, lens_char\n","    \n","def get_subword_statistics(reference_file_name, source_file_name, target_file_name, file_align_dirs, file_token_source_dirs, file_token_target_dirs):\n","    stats = {}\n","    \n","    vocs_src = []\n","    vocs_tgt = []\n","    file_align_dict = {}\n","    with open(file_align_dirs, 'r') as f:\n","        for line in f:\n","            voc_src = line.split()[0].split('-')[0]\n","            voc_tgt = line.split()[0].split('-')[1]\n","            if voc_src not in vocs_src + ['char', 'word']:\n","                vocs_src.append(voc_src)\n","            if voc_tgt not in vocs_tgt+ ['char', 'word']:\n","                vocs_tgt.append(voc_tgt)\n","\n","            file_align_dict[line.split()[0]] = line.split()[1]\n","\n","    vocs_src= [int(v) for v in vocs_src]\n","    vocs_tgt= [int(v) for v in vocs_tgt]\n","\n","    vocs_src.sort()\n","    vocs_tgt.sort()\n","\n","    vocs_src= ['char'] + [str(v) for v in vocs_src] + ['word']\n","    vocs_tgt= ['char'] + [str(v) for v in vocs_tgt] + ['word']\n","\n","    file_src_dict = {}\n","    with open(file_token_source_dirs, 'r') as f:\n","        for line in f:\n","            file_src_dict[line.split()[0]] = line.split()[1]\n","\n","    file_tgt_dict = {}\n","    with open(file_token_target_dirs, 'r') as f:\n","        for line in f:\n","            file_tgt_dict[line.split()[0]] = line.split()[1]\n","\n","    reference_set, sure_set, fuzzy_set = get_reference(reference_file_name, ref_only=False)\n","    source_set = get_corpus_file(source_file_name)\n","    target_set = get_corpus_file(target_file_name)\n","\n","    def get_analyse_prediction(vocab_pair, prediction_set_, reference_set, sure_set, fuzzy_set, source_set, target_set):\n","        prediction_set = get_prediction(prediction_set_)\n","        statsPre = analyse_prediction(prediction_set, reference_set, sure_set, fuzzy_set, source_set, target_set)\n","\n","        voc_src = vocab_pair.split('-')[0]\n","        voc_tgt = vocab_pair.split('-')[1]\n","\n","        len_word_source, len_char_source = get_len(file_src_dict[voc_src])\n","        len_word_target, len_char_target = get_len(file_tgt_dict[voc_tgt])\n","        \n","        statsPre[\"len_word_source\"] = len_word_source\n","        statsPre[\"len_char_source\"] = len_char_source\n","        statsPre[\"len_word_target\"] = len_word_target\n","        statsPre[\"len_char_target\"] =len_char_target\n","\n","        print(vocab_pair, end=' ')\n","\n","        return statsPre\n","\n","    with concurrent.futures.ThreadPoolExecutor() as executor:\n","        statsRef= executor.submit(analyse_reference, reference_set, sure_set, fuzzy_set, source_set, target_set)\n","\n","        statsPres = {}\n","        for vocab_pair in file_align_dict:\n","            statsPres[vocab_pair] = executor.submit(get_analyse_prediction, vocab_pair, file_align_dict[vocab_pair], reference_set, sure_set, fuzzy_set, source_set, target_set)\n","\n","        stats[\"reference\"] = statsRef.result()\n","\n","        for vocab_pair in file_align_dict:\n","            stats[vocab_pair] = statsPres[vocab_pair].result()\n","\n","    return stats\n","\n","def get_len_n_gram(file_name):\n","    n_grams = {}\n","    with open(file_name, 'r') as f:\n","        for line in f:\n","            words = line.split()\n","            for word in words:\n","                w = word.replace('▁', '')\n","                len_word = len(w)\n","                if len_word not in n_grams:\n","                    n_grams[len_word] = [w]\n","                else:\n","                    if w not in n_grams[len_word]:\n","                        n_grams[len_word].append(w)\n","\n","    len_n_gram = {}\n","    for n_gram in n_grams:\n","        if n_gram != 0:\n","            len_n_gram[n_gram] = len(n_grams[n_gram])\n","            \n","    return len_n_gram\n","\n","def add_subword_statistics_n_gram(subword_statistic_file_name, file_align_dirs, source_file_name, target_file_name, file_token_source_dirs, file_token_target_dirs):\n","    with open(subword_statistic_file_name, 'r') as f:\n","        stats = json.load(f)\n","    \n","    vocs_src = []\n","    vocs_tgt = []\n","    vocab_pairs = []\n","    with open(file_align_dirs, 'r') as f:\n","        for line in f:\n","            voc_src = line.split()[0].split('-')[0]\n","            voc_tgt = line.split()[0].split('-')[1]\n","            if voc_src not in vocs_src + ['char', 'word']:\n","                vocs_src.append(voc_src)\n","            if voc_tgt not in vocs_tgt+ ['char', 'word']:\n","                vocs_tgt.append(voc_tgt)\n","\n","    vocs_src= [int(v) for v in vocs_src]\n","    vocs_tgt= [int(v) for v in vocs_tgt]\n","\n","    vocs_src.sort()\n","    vocs_tgt.sort()\n","\n","    vocs_src= ['char'] + [str(v) for v in vocs_src] + ['word']\n","    vocs_tgt= ['char'] + [str(v) for v in vocs_tgt] + ['word']\n","\n","    file_src_dict = {}\n","    with open(file_token_source_dirs, 'r') as f:\n","        for line in f:\n","            file_src_dict[line.split()[0]] = line.split()[1]\n","\n","    file_tgt_dict = {}\n","    with open(file_token_target_dirs, 'r') as f:\n","        for line in f:\n","            file_tgt_dict[line.split()[0]] = line.split()[1]\n","    \n","    with concurrent.futures.ThreadPoolExecutor() as executor:\n","        statsRef = stats[\"reference\"]\n","\n","        statsRef['n_gram_src'] = executor.submit(get_len_n_gram, source_file_name).result()\n","        statsRef['n_gram_tgt'] = executor.submit(get_len_n_gram, target_file_name).result()\n","\n","        for voc_src in vocs_src:\n","            for voc_tgt in vocs_tgt:\n","                stats[voc_src+'-'+voc_tgt]['n_gram_src'] = executor.submit(get_len_n_gram, file_src_dict[voc_src]).result()\n","                stats[voc_src+'-'+voc_tgt]['n_gram_tgt'] = executor.submit(get_len_n_gram, file_tgt_dict[voc_tgt]).result()\n","\n","    return stats\n","    \n","def get_data_directory(src, tgt):\n","    main_dir = \"/content/drive/MyDrive/WordAlignmentCorpora/\"\n","    result_dir = \"/content/drive/MyDrive/subwordOptimization/\"\n","    if src == 'en' and tgt == 'ro':\n","        test_src = main_dir + \"en-ro/corp.test.ro-en.cln.en.low\"\n","        test_tgt = main_dir + \"en-ro/corp.test.ro-en.cln.ro.low\"\n","        test_align = main_dir + \"en-ro/test.en-ro.ali.startFrom1\"\n","        align_dir = result_dir + \"en-ro/output_en_ro\"\n","        src_dir = result_dir + \"en-ro/output_en\"\n","        tgt_dir = result_dir + \"en-ro/output_ro\"\n","    if src == 'ro' and tgt == 'en':\n","        test_src = main_dir + \"en-ro/corp.test.ro-en.cln.ro.low\"\n","        test_tgt = main_dir + \"en-ro/corp.test.ro-en.cln.en.low\"\n","        test_align = main_dir + \"en-ro/test.ro-en.ali.startFrom1\"\n","        align_dir = result_dir + \"en-ro/output_ro_en\"\n","        src_dir = result_dir + \"en-ro/output_ro\"\n","        tgt_dir  = result_dir + \"en-ro/output_en\"\n","\n","    if src == 'en' and tgt == 'cz':\n","        test_src = main_dir + \"en-cz/testing.en-cz.en.low\" \n","        test_tgt = main_dir + \"en-cz/testing.en-cz.cz.low\"\n","        test_align = main_dir + \"en-cz/testing.en-cz.alignment.fixed\"\n","        align_dir = result_dir + \"en-cz/output_en_cz\"\n","        src_dir = result_dir + \"en-cz/output_en\"\n","        tgt_dir  = result_dir + \"en-cz/output_cz\"\n","    if src == 'cz' and tgt == 'en':\n","        test_src = main_dir + \"en-cz/testing.en-cz.cz.low\"\n","        test_tgt = main_dir + \"en-cz/testing.en-cz.en.low\"\n","        test_align = main_dir + \"en-cz/testing.cz-en.alignment.fixed\"\n","        align_dir = result_dir + \"en-cz/output_cz_en\"\n","        src_dir = result_dir + \"en-cz/output_cz\"\n","        tgt_dir  = result_dir + \"en-cz/output_en\"\n","\n","    if src == 'en' and tgt == 'fr':\n","        test_src = main_dir + \"en-fr/testing.low.en\"\n","        test_tgt = main_dir + \"en-fr/testing.low.fr\"\n","        test_align = main_dir + \"en-fr/testing.en-fr.align\"\n","        align_dir = result_dir + \"en-fr/output_en_fr\"\n","        src_dir = result_dir + \"en-fr/output_en\"\n","        tgt_dir  = result_dir + \"en-fr/output_fr\"\n","    if src == 'fr' and tgt == 'en':\n","        test_src = main_dir + \"en-fr/testing.low.fr\"\n","        test_tgt = main_dir + \"en-fr/testing.low.en\"\n","        test_align = main_dir + \"en-fr/testing.fr-en.align\"\n","        align_dir = result_dir + \"en-fr/output_fr_en\"\n","        src_dir = result_dir + \"en-fr/output_fr\"\n","        tgt_dir  = result_dir + \"en-fr/output_en\"\n","\n","    if src == 'en' and tgt == 'ge':\n","        test_src = main_dir + \"en-de/corp.test.de-en.en.low.ngoho\"\n","        test_tgt = main_dir + \"en-de/corp.test.de-en.de.low.ngoho\"\n","        test_align = main_dir + \"en-de/alignmentDeEn.fixed.ali.startFrom1.en-de.ngoho\"\n","        align_dir = result_dir + \"en-de/output_en_de\"\n","        src_dir = result_dir + \"en-de/output_en\"\n","        tgt_dir  = result_dir + \"en-de/output_de\"\n","    if src == 'ge' and tgt == 'en':\n","        test_src = main_dir + \"en-de/corp.test.de-en.de.low.ngoho\"\n","        test_tgt = main_dir + \"en-de/corp.test.de-en.en.low.ngoho\"\n","        test_align = main_dir + \"en-de/alignmentDeEn.fixed.ali.startFrom1.de-en.ngoho\"\n","        align_dir = result_dir + \"en-de/output_de_en\"\n","        src_dir = result_dir + \"en-de/output_de\"\n","        tgt_dir  = result_dir + \"en-de/output_en\"\n","    \n","    if src == 'en' and tgt == 'ja':\n","        test_src = main_dir + \"en-ja/testing.en-ja.en\"\n","        test_tgt = main_dir + \"en-ja/testing.en-ja.ja\"\n","        test_align = main_dir + \"en-ja/en-ja.align.from1.final\"\n","        align_dir = result_dir + \"en-ja/output_en_ja\"\n","        src_dir = result_dir + \"en-ja/output_en\"\n","        tgt_dir  = result_dir + \"en-ja/output_ja\"\n","    if src == 'ja' and tgt == 'en':\n","        test_src = main_dir + \"en-ja/testing.en-ja.ja\"\n","        test_tgt = main_dir + \"en-ja/testing.en-ja.en\"\n","        test_align = main_dir + \"en-ja/ja-en.align.from1.final\"\n","        align_dir = result_dir + \"en-ja/output_ja_en\"\n","        src_dir = result_dir + \"en-ja/output_ja\"\n","        tgt_dir  = result_dir + \"en-ja/output_en\"\n","\n","    if src == 'en' and tgt == 'vi':\n","        test_src = main_dir + \"en-vi/testing.en-vi.low.en\"\n","        test_tgt = main_dir + \"en-vi/testing.en-vi.low.vi\"\n","        test_align = main_dir + \"en-vi/testing.en-vi.align.from1\"\n","        align_dir = result_dir + \"en-vi/output_en_vi\"\n","        src_dir = result_dir + \"en-vi/output_en\"\n","        tgt_dir  = result_dir + \"en-vi/output_vi\"\n","    if src == 'vi' and tgt == 'en':\n","        test_src = main_dir + \"en-vi/testing.en-vi.low.vi\"\n","        test_tgt = main_dir + \"en-vi/testing.en-vi.low.en\"\n","        test_align = main_dir + \"en-vi/testing.vi-en.align.from1\"\n","        align_dir = result_dir + \"en-vi/output_vi_en\"\n","        src_dir = result_dir + \"en-vi/output_vi\"\n","        tgt_dir  = result_dir + \"en-vi/output_en\"\n","    \n","    return test_src, test_tgt, test_align, align_dir, src_dir, tgt_dir\n"],"metadata":{"id":"ITI8LWRqoGRf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BqZ-MaMflPdH"},"source":["## Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7Cew62iuU0H"},"outputs":[],"source":["!mkdir /content/drive/MyDrive/subwordOptimization/\n","\n","train_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-ro/train.merg.en-ro.cln.en.utf8.low.lenSent50\"\n","train_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-ro/train.merg.en-ro.cln.ro.utf8.low.lenSent50\"\n","test_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-ro/corp.test.ro-en.cln.en.low\"\n","test_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-ro/corp.test.ro-en.cln.ro.low\"\n","data_dir=\"/content/drive/MyDrive/subwordOptimization/en-ro/\"\n","\n","get_alignment_files(train_src, train_tgt,\n","                        test_src, test_tgt,\n","                        data_dir,\n","                        src='en', tgt='ro',\n","                        model_type='bpe',\n","                        vocab_sizes=[100, 200, 500, 1000, 2000, 4000, 8000, 16000, 32000, 48000])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECLcsmY_N84d"},"outputs":[],"source":["!mkdir /content/drive/MyDrive/subwordOptimization/\n","\n","train_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-ja/kyoto-train.cln.low.en\"\n","train_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-ja/kyoto-train.cln.low.ja\"\n","test_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-ja/testing.en-ja.en\"\n","test_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-ja/testing.en-ja.ja\"\n","data_dir=\"/content/drive/MyDrive/subwordOptimization/en-ja/\"\n","\n","get_alignment_files(train_src, train_tgt,\n","                        test_src, test_tgt,\n","                        data_dir,\n","                        src='en', tgt='ja',\n","                        model_type='bpe',\n","                        vocab_sizes=[100, 200, 500, 1000, 2000, 4000, 8000, 16000, 32000, 48000])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ald4x28huPy1"},"outputs":[],"source":["!mkdir /content/drive/MyDrive/subwordOptimization/\n","\n","train_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-cz/training.en-cz.en.tok.low.cln\"\n","train_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-cz/training.en-cz.cz.tok.low.cln\"\n","test_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-cz/testing.en-cz.en.low\"\n","test_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-cz/testing.en-cz.cz.low\"\n","data_dir=\"/content/drive/MyDrive/subwordOptimization/en-cz/\"\n","\n","get_alignment_files(train_src, train_tgt,\n","                        test_src, test_tgt,\n","                        data_dir,\n","                        src='en', tgt='cz',\n","                        model_type='bpe',\n","                        vocab_sizes=[100, 200, 500, 1000, 2000, 4000, 8000, 16000, 32000, 48000])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDJG2xoua5pj"},"outputs":[],"source":["!mkdir /content/drive/MyDrive/subwordOptimization/\n","\n","train_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-vi/train.low.en\"\n","train_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-vi/train.low.vi\"\n","test_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-vi/testing.en-vi.low.en\"\n","test_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-vi/testing.en-vi.low.vi\"\n","data_dir=\"/content/drive/MyDrive/subwordOptimization/en-vi/\"\n","\n","get_alignment_files(train_src, train_tgt,\n","                        test_src, test_tgt,\n","                        data_dir,\n","                        src='en', tgt='vi',\n","                        model_type='bpe',\n","                        vocab_sizes=[100, 200, 500, 1000, 2000, 4000, 8000, 16000, 32000, 48000])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nK6BHuGjMGeq"},"outputs":[],"source":["!mkdir /content/drive/MyDrive/subwordOptimization/\n","\n","train_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-de/corp.train.de-en.low.cln.en.final.lenSent50\"\n","train_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-de/corp.train.de-en.low.cln.de.final.lenSent50\"\n","test_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-de/corp.test.de-en.en.low.ngoho\"\n","test_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-de/corp.test.de-en.de.low.ngoho\"\n","data_dir=\"/content/drive/MyDrive/subwordOptimization/en-de/\"\n","\n","get_alignment_files(train_src, train_tgt,\n","                        test_src, test_tgt,\n","                        data_dir,\n","                        src='en', tgt='de',\n","                        model_type='bpe',\n","                        vocab_sizes=[100, 200, 500, 1000, 2000, 4000, 8000, 16000, 32000, 48000])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7Buedxfu5JYW"},"outputs":[],"source":["!mkdir /content/drive/MyDrive/subwordOptimization/\n","\n","train_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-fr/europarl-v7.en-fr.cln.low.en.lenSent50\"\n","train_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-fr/europarl-v7.en-fr.cln.low.fr.lenSent50\"\n","test_src = \"/content/drive/MyDrive/WordAlignmentCorpora/en-fr/testing.low.en\"\n","test_tgt = \"/content/drive/MyDrive/WordAlignmentCorpora/en-fr/testing.low.fr\"\n","data_dir=\"/content/drive/MyDrive/subwordOptimization/en-fr/\"\n","\n","get_alignment_files(train_src, train_tgt,\n","                        test_src, test_tgt,\n","                        data_dir,\n","                        src='en', tgt='fr',\n","                        model_type='bpe',\n","                        vocab_sizes=[100, 200, 500, 1000, 2000, 4000, 8000, 16000, 32000, 48000])"]},{"cell_type":"code","source":["lang_pairs = [\n","              {'src':'en', 'tgt':'ro'}, \n","              {'src':'ro', 'tgt':'en'},\n","\n","              {'src':'en', 'tgt':'cz'},\n","              {'src':'cz', 'tgt':'en'},\n","\n","              {'src':'en', 'tgt':'fr'},\n","              {'src':'fr', 'tgt':'en'},\n","\n","              {'src':'en', 'tgt':'ge'},\n","              {'src':'ge', 'tgt':'en'},\n","              \n","              {'src':'en', 'tgt':'ja'},\n","              {'src':'ja', 'tgt':'en'},\n","              \n","              {'src':'en', 'tgt':'vi'},\n","              {'src':'vi', 'tgt':'en'},\n","              ]\n","\n","for lang_pair in lang_pairs:\n","\n","    print(\"Create file\", '/content/drive/MyDrive/subword_stats_'+lang_pair['src']+'_'+lang_pair['tgt'])\n","    test_src, test_tgt, test_align, align_dir, src_dir, tgt_dir = get_data_directory(src=lang_pair['src'], tgt=lang_pair['tgt'])\n","\n","    subword_statistics = get_subword_statistics(test_align, test_src, test_tgt, align_dir, src_dir, tgt_dir)\n","\n","    with open('/content/drive/MyDrive/subword_stats_'+lang_pair['src']+'_'+lang_pair['tgt'], 'w+') as file:\n","        file.write(json.dumps(subword_statistics, indent=4))\n","\n","    print(\"Done\", '/content/drive/MyDrive/subword_stats_'+lang_pair['src']+'_'+lang_pair['tgt'] )\n","        "],"metadata":{"id":"JZsGQHGenbfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9C2R0ZrusU-"},"outputs":[],"source":["import os\n","with open('/content/drive/MyDrive/subwordOptimization/en-ja/output_en_ja', 'r') as f:\n","    for line in f:\n","        if os.path.isfile(line.split()[1]):\n","            file_ = open(line.split()[1], \"r\")\n","            line_count = 0\n","            for line_ in file_:\n","                if line != \"\\n\":\n","                    line_count += 1\n","            file_.close()\n","            if line_count == 0:\n","                print(line_count, line.split()[1])\n","\n","                open(line.split()[1], 'w').close()\n","                os.remove(line.split()[1])\n","\n","        if os.path.isfile(line.split()[1].replace('.2word', '')):\n","            file_ = open(line.split()[1].replace('.2word', ''), \"r\")\n","            line_count = 0\n","            for line_ in file_:\n","                if line != \"\\n\":\n","                    line_count += 1\n","            file_.close()\n","            if line_count == 0:\n","                print(line_count, line.split()[1].replace('.2word', ''))\n","\n","                open(line.split()[1].replace('.2word', ''), 'w').close()\n","                os.remove(line.split()[1].replace('.2word', ''))\n","\n","        if os.path.isfile(line.split()[1].replace('.shift1.2word', '')):\n","            file_ = open(line.split()[1].replace('.shift1.2word', ''), \"r\")\n","            line_count = 0\n","            for line_ in file_:\n","                if line != \"\\n\":\n","                    line_count += 1\n","            file_.close()\n","            if line_count == 0:\n","                print(line_count, line.split()[1].replace('.shift1.2word', ''))\n","\n","                open(line.split()[1].replace('.shift1.2word', ''), 'w').close()\n","                os.remove(line.split()[1].replace('.shift1.2word', ''))\n","\n","                "]}],"metadata":{"colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Word Alignment Subword Optimization Code.ipynb","provenance":[],"mount_file_id":"1QPHunzfUaXmubNjvTpu6ekEjNPNZkS-H","authorship_tag":"ABX9TyNwxtWidqc6FQpEpv2p6Xvj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}